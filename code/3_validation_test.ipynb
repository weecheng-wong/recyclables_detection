{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "052df548",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58e44378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6981665",
   "metadata": {},
   "source": [
    "# Set data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbc7a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder\n",
    "dataset_path = '../data'\n",
    "\n",
    "# train val test annotations\n",
    "train_file_path = dataset_path + '/train/' + 'annotations_0_train.json'\n",
    "val_file_path = dataset_path + '/val/' + 'annotations_0_val.json'\n",
    "test_file_path = dataset_path + '/test/' + 'annotations_0_test.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f240665",
   "metadata": {},
   "source": [
    "# Load validation and test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9c4f6b",
   "metadata": {},
   "source": [
    "Initially, validation dataset is intended to check the scores during training for each epochs.\\\n",
    "Due to training runs crashing, the validation code was omitted.\\\n",
    "'Validation' and 'Test' are used to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9737dbc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "val = dset.CocoDetection(root = dataset_path,\n",
    "                               annFile = val_file_path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0826dde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "test = dset.CocoDetection(root = dataset_path,\n",
    "                               annFile = test_file_path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ae934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select device (whether GPU or CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded122f",
   "metadata": {},
   "source": [
    "# Defining custom metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a057e969",
   "metadata": {},
   "source": [
    "Retrieving images, annotations, labels and scores into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "933dc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(model, val, confidence_threshold=0.5):\n",
    "# val is the Cocodetection dataset\n",
    "\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    \n",
    "    pred_boxes = []\n",
    "    true_boxes = []\n",
    "\n",
    "    idx = 0\n",
    "\n",
    "    for img, anns in val:\n",
    "\n",
    "        # each image can have multiple annotations\n",
    "        # get list of ground truth annotations [ [idx, class_pred, prob_score, x1, y1, x2, y2], ... ]\n",
    "        for ann in anns:\n",
    "\n",
    "            ann_list = []\n",
    "\n",
    "            img_idx_in_val = idx\n",
    "            class_pred = ann['category_id']+1 # shift by +1 to account for addition of background class\n",
    "            prob_score = 1 # ground truth prob is 1\n",
    "            x1 = int(ann['bbox'][0])\n",
    "            y1 = int(ann['bbox'][1])\n",
    "            x2 = int(ann['bbox'][2] + x1)\n",
    "            y2 = int(ann['bbox'][3] + y1)\n",
    "\n",
    "            ann_list = [img_idx_in_val, class_pred, prob_score, x1, y1, x2, y2]\n",
    "            true_boxes.append(ann_list)\n",
    "        \n",
    "        model_eval = model.eval()\n",
    "        # get model detections\n",
    "        detections = model_eval(to_tensor(img).to(device).unsqueeze(0))\n",
    "\n",
    "        # set condition to return only scores above 0.5\n",
    "        mask = detections[0]['scores']>confidence_threshold                    \n",
    "\n",
    "        # length of results with scores more than 0.5\n",
    "        length = len(detections[0]['scores'][mask])\n",
    "\n",
    "        for det in range(length):\n",
    "\n",
    "            det_list = []\n",
    "\n",
    "            img_idx_in_val = idx\n",
    "            class_pred = detections[0]['labels'][det].item()\n",
    "            prob_score = detections[0]['scores'][det].item()\n",
    "            x1 = int(detections[0]['boxes'][det][0].item())\n",
    "            y1 = int(detections[0]['boxes'][det][1].item())\n",
    "            x2 = int(detections[0]['boxes'][det][2].item())\n",
    "            y2 = int(detections[0]['boxes'][det][3].item())\n",
    "\n",
    "            det_list = [img_idx_in_val, class_pred, prob_score, x1, y1, x2, y2]\n",
    "            pred_boxes.append(det_list)\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    return true_boxes, pred_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ec2b5",
   "metadata": {},
   "source": [
    "Defining IOU metric to determine the confidence of predicted bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33f425dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
    "    if interArea == 0:\n",
    "        return 0\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
    "    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35223830",
   "metadata": {},
   "source": [
    "Define function to compute the custom metric for bounding box detection rate vs ground truth and correct classification rate vs ground truth for each image.\\\n",
    "Conventional mean average precision uses the average precision for each class, but unable to install 'FiftyOne' library or develop that code in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeee0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_metric(val, true_boxes, pred_boxes):\n",
    "\n",
    "    correct_detection_rate = []\n",
    "    class_detection_rate = []\n",
    "\n",
    "    for i in range(len(val)):\n",
    "\n",
    "        # for every test image\n",
    "\n",
    "        each_image_true = []\n",
    "        for t_box in true_boxes:\n",
    "            if t_box[0] == i:\n",
    "                each_image_true.append(t_box)\n",
    "\n",
    "        each_image_pred = []\n",
    "        for p_box in pred_boxes:\n",
    "            if p_box[0] == i:\n",
    "                each_image_pred.append(p_box)\n",
    "\n",
    "        num_true = len(each_image_true)\n",
    "        correct_detection = 0\n",
    "        class_detection = 0\n",
    "        pos_class_detection = 0\n",
    "        neg_class_detection = 0\n",
    "\n",
    "        for t_item in each_image_true:\n",
    "            true_box = t_item[3:]\n",
    "            true_class = t_item[1]\n",
    "\n",
    "            good_iou_counts = 0\n",
    "            bad_iou_counts = 0\n",
    "\n",
    "            for p_item in each_image_pred:\n",
    "                pred_box = p_item[3:]\n",
    "                pred_class = p_item[1]\n",
    "\n",
    "                iou = intersection_over_union(true_box, pred_box)\n",
    "                if iou >= 0.5:\n",
    "                    good_iou_counts += 1\n",
    "                    if pred_class == true_class:\n",
    "                        pos_class_detection += 1\n",
    "                    else:\n",
    "                        neg_class_detection += 1\n",
    "                elif iou < 0.5:\n",
    "                    bad_iou_counts += 1\n",
    "\n",
    "            if good_iou_counts >= 1:\n",
    "                correct_detection += 1\n",
    "\n",
    "            if pos_class_detection >= 1:\n",
    "                class_detection += 1\n",
    "\n",
    "        detection_rate = correct_detection / num_true\n",
    "\n",
    "        correct_detection_rate.append(detection_rate)\n",
    "\n",
    "        class_rate = class_detection / num_true\n",
    "\n",
    "        class_detection_rate.append(class_rate)\n",
    "    \n",
    "    print(f'bounding boxes detection rate: {np.mean(correct_detection_rate)*100:.0f}%')\n",
    "    print(f'class detection rate: {np.mean(class_detection_rate)*100:.0f}%')\n",
    "#     return np.mean(correct_detection_rate), np.mean(class_detection_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d40568",
   "metadata": {},
   "source": [
    "# Model metric for Faster RCNN with Mobilenetv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "919e7875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 2 classes; Only target class or background\n",
    "num_classes = 61\n",
    "\n",
    "model_mobnet = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# stop the model from keeping gradients\n",
    "for param in model_mobnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# move model to the right device\n",
    "model_mobnet.to(device)\n",
    "\n",
    "PATH = 'mobnet_model_25_34_cuda.pt'\n",
    "\n",
    "checkpoint = torch.load(PATH, map_location=device)\n",
    "model_mobnet.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbca193",
   "metadata": {},
   "source": [
    "For validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "210b751a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounding boxes detection rate: 36%\n",
      "class detection rate: 29%\n"
     ]
    }
   ],
   "source": [
    "tb_mn, pb_mn = get_boxes(model_mobnet, val, confidence_threshold=0.5)\n",
    "model_metric(val, tb_mn, pb_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eadfe96",
   "metadata": {},
   "source": [
    "For test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69bb191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounding boxes detection rate: 33%\n",
      "class detection rate: 31%\n"
     ]
    }
   ],
   "source": [
    "tb_mn, pb_mn = get_boxes(model_mobnet, test, confidence_threshold=0.5)\n",
    "model_metric(test, tb_mn, pb_mn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748bdd2",
   "metadata": {},
   "source": [
    "# Model metric for Faster RCNN with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad42788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    # load an instance segmentation model pre-trained pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "\n",
    "# 2 classes; Only target class or background\n",
    "num_classes = 61\n",
    "\n",
    "model_fr = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# stop the model from keeping gradients\n",
    "for param in model_mobnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# move model to the right device\n",
    "model_fr.to(device)\n",
    "\n",
    "PATH = 'model_35_39.pt'\n",
    "\n",
    "checkpoint = torch.load(PATH, map_location=device)\n",
    "model_fr.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef665ff",
   "metadata": {},
   "source": [
    "For validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afafcd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounding boxes detection rate: 40%\n",
      "class detection rate: 36%\n"
     ]
    }
   ],
   "source": [
    "tb_fr, pb_fr = get_boxes(model_fr, val, confidence_threshold=0.5)\n",
    "model_metric(val, tb_fr, pb_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf550e5e",
   "metadata": {},
   "source": [
    "For test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b70c8982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounding boxes detection rate: 40%\n",
      "class detection rate: 40%\n"
     ]
    }
   ],
   "source": [
    "tb_fr, pb_fr = get_boxes(model_fr, test, confidence_threshold=0.5)\n",
    "model_metric(test, tb_fr, pb_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1dfc8e",
   "metadata": {},
   "source": [
    "# Model metric for RetinaNet with ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e6620bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "model_rn = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# move model to the right device\n",
    "model_rn.to(device)\n",
    "    \n",
    "# stop the model from keeping gradients\n",
    "for param in model_mobnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "PATH = 'retnet_model_30_39.pt'\n",
    "\n",
    "checkpoint = torch.load(PATH, map_location=device)\n",
    "model_rn.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52863a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounding boxes detection rate: 43%\n",
      "class detection rate: 31%\n"
     ]
    }
   ],
   "source": [
    "tb_rn, pb_rn = get_boxes(model_rn, val, confidence_threshold=0.5)\n",
    "model_metric(val, tb_rn, pb_rn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c366decb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounding boxes detection rate: 41%\n",
      "class detection rate: 38%\n"
     ]
    }
   ],
   "source": [
    "tb_rn, pb_rn = get_boxes(model_rn, test, confidence_threshold=0.5)\n",
    "model_metric(test, tb_rn, pb_rn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
